---
title: "Machine Learning"
subtitle: "Assignment 3, Group Assignment"
author: "Group 1: Siow Meng Low, Louise Fallon, Nikhita Venkatesan, George Pastakas, Cecilia Nok Sze Cheung, Steven Locorotondo"
output: pdf_document
---

```{r setup, include=FALSE}
library(gmodels)
library(knitr)
library(C50)
library(reshape)
library(ggplot2)
library(ggthemes)

knitr::opts_chunk$set(echo = FALSE)
##set seed to ensure random processes run consistently
set.seed(99)
```

## Q1: Import Pre-Processed Data and Splitting

The loans data has been preprocessed using Python code provided. There is a slight amendment to the Python file "cleanup.py": the first row of the CSV file is not required, hence we set *skiprows = 1* in *read_csv()* function. The modified Python code is submitted as well.  

We first read the `.csv` file which contains the pre-processed data with 8 columns in total, including the outcomes of the loans, which can be either *"Charged off"* or *"Fully Paid"*.

```{r}
# Load the data file
processedLoans <- read.csv("Loans_processed.csv")
```

Next, we split the data into: 

* Training Set (20,000 records) 
* Validation Set (8,000 records) 
* Test Set (Remaining 10,697 records) 

```{r}
# Q1
# Separate data into training (20,000 records), validation (8,000 records), and test (remaining records)

inTrain <- sample.int(nrow(processedLoans), size = 20000)

train <- processedLoans[inTrain, ]
others <- processedLoans[-inTrain, ]

inValidation <- sample.int(nrow(others), size = 8000)

validation <- others[inValidation, ]
test <- others[-inValidation, ]
```

## Q2: Classification Tree Using C50

```{r}
# Q2
# CrossTable(processedLoans$loan_status, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, 
#           dnn = c("Actual Status"))
t <- table(processedLoans$loan_status)
# number of repaid loans / number of total loans
propRepaid <- nrow(processedLoans[processedLoans$loan_status == "Fully Paid", ]) / nrow(processedLoans)
```

The total number of loans in the data that fall into each category is

Table: Number of instances in each class

| Charged Off | Fully Paid |
|:-----------:|:----------:|
| `r t[1]`    | `r t[2]`   |

The proportion of repaid loans in the dataset is `r format(propRepaid, digits = 4)`. We will now try to achieve an accuracy greater than this using classification trees. We first train a decision tree based on the training set. 

```{r}
treeModel <- C5.0(x = subset(train, select = -loan_status), y = train$loan_status)
#treeModel
#summary(treeModel)

# rows represent actual outcome, columns represent predicted outcome
confTrain <- table(train$loan_status, predict(treeModel, subset(train, select = -loan_status)))
```

The confusion matrix of the training set for the classifier we trained is

Table: Confusion Matrix for Training Set

|*actual / predicted*| Charged Off | Fully Paid |
|------------------|:-----------:|:----------:|
| Charged Off      | `r confTrain[1]` | `r confTrain[3]` |
| Fully Paid       | `r confTrain[2]` | `r confTrain[4]` |

As observed, the classification tree classifies all training samples as *"Fully Paid"* and its accuracy is simply the proportion of *"Fully Paid"* loans in the training set, which in this case is `r format(length(train$loan_status[train$loan_status=="Fully Paid"])/length(train$loan_status),digits=4)`, close to the total proportion of repaid loans. 

We will now use the classifier to predict the outcome of each lean in the validation set.

```{r}
confMatDims <- list(c("Charged Off", "Fully Paid"), c("Charged Off", "Fully Paid"))

confVal <- table(validation$loan_status, predict(treeModel, subset(validation, select = -loan_status)))
#CrossTable(validation$loan_status, predict(treeModel, subset(validation, select = -loan_status)), 
#           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c("Actual Status", "Predicted Status"))

```

The resulting confusion matrix of the validation set for the classifier is

Table: Confusion Matrix for Validation Set

|*actual / predicted*| Charged Off    | Fully Paid     |
|--------------------|:--------------:|:--------------:|
| Charged Off        | `r confVal[1]` | `r confVal[3]` |
| Fully Paid         | `r confVal[2]` | `r confVal[4]` |

Again, all the validation records are predicted to be *"Fully Paid"*. Hence, its accuracy is the proportion of *"Fully Paid"* loans in the validation set, which in this case is `r format(length(validation$loan_status[validation$loan_status=="Fully Paid"])/length(validation$loan_status),digits=4)`, close to the total proportion of repaid loans. 

To understand this behaviour, we will need to refer to the default **[pruning behaviour of C5.0](http://www.rulequest.com/see5-win.html#OTHER)**. A subtree will be pruned if it has an error estimate higher than the "CF Option", with a default value 0.25 (this value can be set using *C5.0Control()*). A higher value of "CF Option" might increase the tendency of overfitting to training data.  

As we know, the number of *"Fully Paid"* loans greatly outnumbers the number of *"Charged Off"* loans. Consequently, during the training phase, the algorithm discovers that further splitting the tree (using any of the seven features) does not reduce the error estimate of the subtree to below 0.25: even though the algorithm could potentially perform splitting such that *"Charged Off"* are majority categories in certain leaf nodes, this improvement is insignificant when compared to the error estimate of the subtree (due to the fact that *"Fully Repaid"* loans are the predominant class in most of the nodes).  

Consequently, the algorithm prunes away all the subtrees and we are only left with one root node. Thus, the trained classification tree simply predicts all loans as *"Fully Paid"*.  

## Q3: C50 Classification Tree with Costs Adjustments  

We will need to train the classification tree with a cost matrix to correct the default behaviour of `C5.0` in maximising the accuracy.  

Granting a loan to a customer who is likely to default is much more costlier (i.e. False Negative) than denying loan to a customer who is likely able to pay back (i.e. False Positive). In the cost matrices, we set the cost of False Positive to $1$ and test out different values of the cost of False Negative (ranging from $2.8$ to $5.2$, in increment of $0.1$). The cost matrix is represented in the table below (with 'X' indicating the varying cost of False Negative which we would like to calibrate):  

Table: Cost Matrix Representation

|*actual / predicted*| Charged Off | Fully Paid |
|------------------|:-----------:|:----------:|
| Charged Off      | 0 | X |
| Fully Paid       | 1 | 0 |

Note that in the above representation, columns are the predicted classes and rows are the actual classes. While passing in to C5.0 function in the R code, we use the transpose of this matrix since R function requires the cost matrix to be represented in the other way (i.e. columns correspond to actual classes and rows correspond to predicted classes).  

```{r}
# Q3
costsMatDims <- list(c("Charged Off", "Fully Paid"), c("Charged Off", "Fully Paid"))
names(costsMatDims) <- c("predicted", "actual")

costsMatList <- list()
confMatList <- list()
validationPerf <- data.frame(weight = NULL, sens = NULL, prec = NULL, spec = NULL, acc = NULL)
for (i in seq(2.8, 5.2, by = 0.1)) {
    
    costsMat <- matrix(c(0, i, 1, 0), nrow = 2, dimnames = costsMatDims)
    costsTree <- C5.0(x = subset(train, select = -loan_status), y = train$loan_status, costs = costsMat)
    #summary(costsTree)
    costsTreeTable <- table(validation$loan_status, 
                            predict(costsTree, subset(validation, select = -loan_status)))
    dimnames(costsTreeTable) <- confMatDims
    
    costsMatList[[length(costsMatList) + 1]] <- costsMat
    confMatList[[length(confMatList) + 1]] <- costsTreeTable
    validationPerf <- rbind(validationPerf, 
                            data.frame(weight = i, 
                                       sens = costsTreeTable["Charged Off", "Charged Off"] / 
                                           rowSums(costsTreeTable)["Charged Off"],
                                       prec = costsTreeTable["Charged Off", "Charged Off"] / 
                                           colSums(costsTreeTable)["Charged Off"],
                                     # spec = costsTreeTable["Fully Paid", "Fully Paid"] / 
                                     #     rowSums(costsTreeTable)["Fully Paid"], 
                                       acc = sum(diag(costsTreeTable)) / sum(costsTreeTable)))
    rownames(validationPerf) <- NULL
    
}

kable(round(validationPerf, 4), 
      caption = "Validation Performance Using Different Cost Matrices", 
      col.names = c("False Negative Cost", "Sensitivity", 
                    "Precision", "Accuracy"), align = "c")
```

From the previous table we observe that as the cost of False Negative increases, the overall accuracy and the precision of the classifier decreases while its sensitivity increases.

```{r, fig.align="center", fig.height=3, fig.width=6}
vPerf <- melt(validationPerf, id.vars = "weight")
ggplot(vPerf) +
    geom_line(aes(x = weight, y = value, color = variable)) +
    geom_hline(yintercept = .25, linetype = 3, alpha = .5) +
    geom_hline(yintercept =  .4, linetype = 3, alpha = .5) +
    geom_hline(yintercept =  .5, linetype = 3, alpha = .5) +
    theme_tufte() +
    scale_color_manual(values = c("#E69F00", "#56B4E9", "black"), 
                       name = "Performance\nMeasures",
                       breaks = c("sens", "prec", "acc"),
                       labels = c("Sensitivity", "Precision", "Accuracy")) +
    labs(x = "False Negative Cost", y = "Percentage (%)") +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .2), labels = seq(0, 1, .2) * 100)

```

The sensitivity levels we are interested in are 25%, 40% and 50%.  

### 25% Sensitivity  

The costs matrix which achieves sensitivity as close as possible to 25% is

```{r}
idx <- which.min(abs(validationPerf$sens - 0.25)) # The index with sensitivity closest to 0.25
cm <- t(costsMatList[[idx]])
```

Table: Cost Matrix for 25% Sensitivity

|*actual / predicted*| Charged Off | Fully Paid |
|------------------|:-----------:|:----------:|
| Charged Off      | `r cm[1]` | `r cm[3]` |
| Fully Paid       | `r cm[2]` | `r cm[4]` |

From the previous table, we can see that this specific cost matrix achieves sensitivity of `r format(validationPerf$sens[idx], digits = 4)` and precision of `r format(validationPerf$prec[idx], digits = 4)`. 

### 40% Sensitivity  

The costs matrix which achieves sensitivity as close as possible to 40% is

```{r}
idx <- which.min(abs(validationPerf$sens - 0.4)) # The index with sensitivity closest to 0.4
cm <- t(costsMatList[[idx]])
```

Table: Cost Matrix for 40% Sensitivity

|*actual / predicted*| Charged Off | Fully Paid |
|------------------|:-----------:|:----------:|
| Charged Off      | `r cm[1]` | `r cm[3]` |
| Fully Paid       | `r cm[2]` | `r cm[4]` |

From the previous table, we can see that this specific cost matrix achieves sensitivity of `r format(validationPerf$sens[idx], digits = 4)` and precision of `r format(validationPerf$prec[idx], digits = 4)`. 

### 50% Sensitivity  

The costs matrix which achieves sensitivity as close as possible to 50% is

```{r}
idx <- which.min(abs(validationPerf$sens - 0.5)) # The index with sensitivity closest to 0.5
cm <- t(costsMatList[[idx]])
```

Table: Cost Matrix for 50% Sensitivity

|*actual / predicted*| Charged Off | Fully Paid |
|------------------|:-----------:|:----------:|
| Charged Off      | `r cm[1]` | `r cm[3]` |
| Fully Paid       | `r cm[2]` | `r cm[4]` |

From the previous table, we can see that this specific cost matrix achieves sensitivity of `r format(validationPerf$sens[idx], digits = 4)` and precision of `r format(validationPerf$prec[idx], digits = 4)`. 

## Q4: Cost Parameter Matrix for Identifying Dubious Loan Applications  

The bank could use the classification tree to predict the loans that are potentially risky. The loan officers can then follow up by double-checking which of those are truly likely to be *"Charged Off"* in future.  

To lower the credit risk, we will need to identify as many truly dubious loan applicants as possible. However, we can see in the earlier section, in order to reach 50% sensitivity level, the precision performance is very low. This means that out of all those applications that are highlighted as risky, only around `r format(validationPerf$prec[idx] * 100, digits = 2)`% are indeed *"Charged Off"* in the validation set. The drawback in reaching a sensitivity level of close to 50% is that the loan officers will need to manually cross-check large number of applications before 50% of the truly risky loans are identified.  

Nevertheless, the cost of granting a loan to risky applicants still far outweighs this labour cost. Hence we pick the cost matrix which achieves sensitivity close to 50%.  

## Q5: Test Set Performance

Using the cost matrix for 50% sensitivity, we retrain the classification tree using both training and validation data. By retraining the model using both training and validation set, the future performance can be more accurately estimated (compared to the scenario where only the training set is used), because we will eventually train the model on the full sample of data to maximise its performance.  

The retrained model is then tested against the test set. Below tabulates the test set performance and confusion matrix. 

```{r, results="hide"}
# Q4
# Choose the costs matrix which yield sensitivity closest to 0.5
idx <- which.min(abs(validationPerf$sens - 0.5)) # The index with sensitivity closest to 0.5

# Q5
# Retrain using training and validation set, and then test on test set
trainValid <- rbind(train, validation)
costsTree2 <- C5.0(x = subset(trainValid, select = -loan_status), 
                   y = trainValid$loan_status, 
                   costs = costsMatList[[idx]])
#summary(costsTree2)
costsTreeTable2 <- table(test$loan_status, 
                         predict(costsTree2, 
                                 subset(test, select = -loan_status)))
dimnames(costsTreeTable2) <- confMatDims

CT <- CrossTable(test$loan_status, predict(costsTree2, 
                                           subset(test, select = -loan_status)), 
                 prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, 
                 dnn = c("Actual Status", "Predicted Status"))

testPerf <- data.frame(weight = validationPerf$weight[idx], 
                       sens = costsTreeTable2["Charged Off", "Charged Off"] / 
                           rowSums(costsTreeTable2)["Charged Off"], 
                       prec = costsTreeTable2["Charged Off", "Charged Off"] / 
                           colSums(costsTreeTable2)["Charged Off"], 
                     # spec = costsTreeTable2["Fully Paid", "Fully Paid"] / 
                     #     rowSums(costsTreeTable2)["Fully Paid"], 
                       acc = sum(diag(costsTreeTable2)) / sum(costsTreeTable2))
```

The final performance of the test set, using the cost matrix that previously resulted in 50% sensitivity, is 

```{r}
kable(round(testPerf[, 2:4], 4), caption = "Test Set Performance", col.names = c("Sensitivity", "Precision", "Accuracy"), row.names = FALSE)
```

The resulting confusion matrix for the test set is

Table: Confusion Matrix for Test Set

|*actual / predicted*| Charged Off    | Fully Paid     |*Total*                    |
|--------------------|:--------------:|:--------------:|:-------------------------:|
| Charged Off        | `r CT[[1]][1]` | `r CT[[1]][3]` |*`r rowSums(CT[[1]])[[1]]`*|
| Fully Paid         | `r CT[[1]][2]` | `r CT[[1]][4]` |*`r rowSums(CT[[1]])[[2]]`*|
|*Total*             |*`r colSums(CT[[1]])[[1]]`*|*`r colSums(CT[[1]])[[2]]`*|*`r sum(CT[[1]])`*|

