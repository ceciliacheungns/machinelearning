{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "### Assignment 2\n",
    "\n",
    "*07 Febraury, 2017*  \n",
    "*Georgios Pastakas*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Assignment: Creating a SMS Message Spam Filter\n",
    "\n",
    "In this assignment we will use the data file [SMSSpamCollection](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/) which contains SMS messages and categorises them in two types, *ham* and *spam*. Using this data, we will build a Naive Bayes spam filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set of symbols we want to exclude from the messages\n",
    "exclude = set(string.punctuation + string.digits + \"£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** First, we load the data into a Python data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./smsspamcollection/SMSSpamCollection\", names = ['Type', 'Message'], sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Before moving on, we will pre-process the SMS messages, by:\n",
    "\n",
    "1. Removing all punctuation and numbers from the SMS messages\n",
    "2. Changing all messages to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"Clear Message\"] = list(map(lambda msg: ''.join(ch for ch in msg if ch not in exclude).lower(), data[\"Message\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Next, we shuffle the messages and split them into\n",
    "\n",
    "* a training set (2,500 messages)\n",
    "* a validation set (1,000 messages) and \n",
    "* a test set (2,072 messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly select 3,500 messages out of the 5,572 and use the rest 2,072 messages as \"test\" set\n",
    "train_and_validation, test = train_test_split(data, train_size = 3500, random_state = 42)\n",
    "\n",
    "# Split \"train_and_validation\" set to \"train\" set and \"test\" set\n",
    "train, validation = train_test_split(train_and_validation, train_size = 2500, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** While Python’s SciKit-Learn library has a Naive Bayes classifier, it works with continuous probability distributions and assumes numerical features. Although it is possible to transform categorical variables into numerical features using a binary encoding, we will instead build a simple Naive Bayes classifier from scratch, which includes the following functions:\n",
    "\n",
    "* `train()`\n",
    "* `train2()`\n",
    "* `predict()`\n",
    "* `score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesForSpam:\n",
    "  \n",
    "    def train(self, hamMessages, spamMessages):\n",
    "        self.words = set(' '.join(hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)) / (len(hamMessages) + len(spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages)\n",
    "            self.likelihoods.append([min(prob1, 0.95), min(prob2, 0.95)])\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "\n",
    "    def train2(self, hamMessages, spamMessages):\n",
    "        self.words = set(' '.join(hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)) / (len(hamMessages) + len(spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages)\n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append([min(prob1, 0.95), min(prob2, 0.95)])\n",
    "                spamkeywords.append(w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "    \n",
    "    def predict(self, message):\n",
    "        posteriors = np.copy(self.priors)\n",
    "        for i, w in enumerate(self.words):\n",
    "            if w in message.lower(): # convert to lower-case\n",
    "                posteriors *= self.likelihoods[:, i]\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:, i]\n",
    "            posteriors = posteriors / np.linalg.norm(posteriors, ord = 1) # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]\n",
    "    \n",
    "    def score(self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape(2, 2)\n",
    "        for m, l in zip(messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0, 0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0, 1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1, 0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1, 1] += 1\n",
    "        return (confusion[0, 0] + confusion[1, 1]) / float(confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** The functions used in `class NaiveBayesForSpam` are:\n",
    "\n",
    "#### `train()`\n",
    "\n",
    "The `train()` function takes as arguments two lists, `hamMessages` and `spamMessages` that contain the messages that are ham and spam, respectively. It merges these two lists and creates a set of all the words contained in all messages.\n",
    "\n",
    "Next, it calculates the prior probabiilites $P(ham)$ and $P(spam)$ of having a ham or a spam message. The results are stored in a list of length 2, names `priors`.\n",
    "\n",
    "After that, it calculates for each word $W$ in the set of words, the likelihood of each word included in a message taking into account that the message is ham or spam, those are $P(W \\mid ham)$ and $P(W \\mid spam)$ for each $W$. It also adds 1 to the number of occurences of each word in hamd and spam emails to avoid having probabilites equal to zero (Laplace estimator). Finally, it restricts the values of the calculated probabilities up to 0.95 by replacing probabilities larger than this value with 0.95. The result is stored in the list `likelihoods` of length equal to the number of unique words.\n",
    "\n",
    "#### `train2()`\n",
    "\n",
    "The `train2()` function takes the same arguments as function `train()` and has the same purpose with only difference the fact that it takes into account only the words from the innitial set of words that have a probability of encountered in spam mails 20 times higher than a probability of encountered in ham mails, this is $20 \\times P(W \\mid ham) < P(W \\mid spam)$. Again, it restricts the likelihood values of these words up to 0.95 by replacing probabilities larger than this value with 0.95. It also creates an additional list, named `spamkeywords`, which includes all these words.\n",
    "\n",
    "####  `predict()`\n",
    "\n",
    "The `predict()` function takes as argument a string object, `message`, which represents a new SMS message that needs to be classified as ham or spam. First, it makes a copy of the prior probabilities $P(ham)$ and $P(spam)$, names `posteriors`, which will constitute the posterior probabilities of a new message being ham or span, these are\n",
    "\n",
    "$$P(ham \\mid message) = \\frac{P(message \\mid ham) \\times P(ham)}{P(message)}$$\n",
    "\n",
    "and\n",
    "$$P(spam \\mid message) = \\frac{P(message \\mid spam) \\times P(spam)}{P(message)}$$\n",
    "\n",
    "where \n",
    "$$P(message \\mid ham) = P(W_1 \\mid ham) \\times P(W_2 \\mid ham) \\times \\cdots \\times P(W_n \\mid ham)$$\n",
    "\n",
    "and\n",
    "$$P(message \\mid spam) = P(W_1 \\mid spam) \\times P(W_2 \\mid spam) \\times \\cdots \\times P(W_n \\mid spam)$$\n",
    "\n",
    "if the message contains all words $W_1, W_2, ..., W_n$. For the words $W_i$ that are not contained in the message $P(W_i)$ are replaced by $P(\\neg W_i) = 1 - P(W_i)$. \n",
    "\n",
    "The function does this computation by initialising $P(ham \\mid message) = P(ham)$ and $P(spam \\mid message) = P(spam)$. Then it iterates through all the words we have in our classifier and checks whether each word $W_i$ exists in the new message or not. If so, it multiplies posterior probabilities with $P(W_i \\mid ham)$ and $P(W_i \\mid spam)$ otherwise it multiplies them with $P(\\neg W_i \\mid ham) = 1 - P(ham \\mid W_i)$ and $P(\\neg W_i \\mid spam) = 1 - P(spam \\mid W_i)$, respectively. After that, it divides both posterior probabilities with the normalised value of the two posterior probabilites which is\n",
    "\n",
    "$$P(message) = P(message \\mid ham) \\times P(ham) + P(message \\mid spam) \\times P(spam)$$\n",
    "\n",
    "The function, up to that point, applies Bayes' Theorem. At the end, if the first posterior probability is greater than 0.5, this is $P(message \\mid ham) > 0.5$, the function returns the predicted class of the new message which is ham and its posterior probability of being a ham. Otherwise, this is $P(message \\mid ham) \\leq 0.5$, the function returns spam as the class of the new message and its posterior probability of being spam.\n",
    "\n",
    "#### `score()`\n",
    "\n",
    "The `score()` function takes as arguments two lists, `messages` and `labels` that contain the messages we have predicted and the true labels of them, respectively. It first creates a $2 \\times 2$ matrix of zeros, which constitutes the confusion matrix. Then, for each pair of message and true value, it compares the predicted value of the message and its true values and increase the corresponding element of the confusion matrix. That is, if\n",
    "\n",
    "* $predicted = ham$ and $actual = ham$: Increase ***True Negatives*** (***TN***) by 1\n",
    "* $predicted = ham$ and $actual = spam$: Increase ***False Negatives*** (***FN***) by 1 \n",
    "* $predicted = spam$ and $actual = ham$: Increase ***False Positives*** (***FP***) by 1 \n",
    "* $predicted = spam$ and $actual = spam$: Increase ***True Positives*** (***TP***) by 1\n",
    "\n",
    "Finally, the function returns both the confusion matrix and the accuracy of the model, which is calculated as\n",
    "\n",
    "$$accuracy = \\frac{TN + TP}{TN + FN + FP + TP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** Use your training set to train the classifiers `train()` and `train2()`. Note that the interfaces of our classifiers require you to pass the ham and spam messages separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate messages of training set to hams and spams\n",
    "train_hams = list(train[train[\"Type\"] == \"ham\"][\"Clear Message\"])\n",
    "train_spams = list(train[train[\"Type\"] == \"spam\"][\"Clear Message\"])\n",
    "\n",
    "# Build Naive Bayes classifier using \"train()\" function\n",
    "NB_1 = NaiveBayesForSpam()\n",
    "train_likelihoods = NB_1.train(train_hams, train_spams)\n",
    "\n",
    "# Build Naive Bayes classifier using \"train2()\" function\n",
    "NB_2 = NaiveBayesForSpam()\n",
    "train_likelihoods = NB_2.train2(train_hams, train_spams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.** After training the two classifiers, we will explore how each of them performs out of sample by using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get messages and labels of the validation set \n",
    "train_messages = list(train[\"Clear Message\"])\n",
    "train_labels = list(train[\"Type\"])\n",
    "\n",
    "# Get messages and labels of the validation set \n",
    "validation_messages = list(validation[\"Clear Message\"])\n",
    "validation_labels = list(validation[\"Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execution time for \"train()\" classifier on the training set\n",
    "acc_train_1, cm_train_1 = NB_1.score(train_messages, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execution time for \"train()\" classifier on the validation set\n",
    "acc_validation_1, cm_validation_1 = NB_1.score(validation_messages, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execution time for \"train2()\" classifier on the training set\n",
    "acc_train_2, cm_train_2 = NB_2.score(train_messages, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execution time for \"train2()\" classifier on the validation set\n",
    "acc_validation_2, cm_validation_2 = NB_2.score(validation_messages, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Comment on accuracy of validation set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.** After using the two classifiers on both the training and the validation set, we wil efirstly valuate them both on their execution time and their accuracy.\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "> *Comment on accuracy in general*\n",
    "\n",
    "#### Execution Time\n",
    "\n",
    "The execution time of `train()` classifier is 00:11:35 for the training set of 2,500 messages and 00:04:40 for the validation set of 1,000 messages while for `train2()` classifier the execution time is is 00:00:29 for the training set and 00:00:11 for the validation set. We see that `train2()` classifier requires significantly less time to be executed than `train()` classifier does. \n",
    "\n",
    "This difference in execution times is something we expected, as in the `train2()` classifier, the words we use to find the posterior probabilities of a new message being ham or spam are only those whose probabilities satisfy inequality $20 \\times P(W \\mid ham) < P(W \\mid spam)$, which are far less that the total number of total words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.** We will now look at speicific classification results. More precisely we will compare the false positives (ham messages classified as spam  messages) and false negatives (spam messages classified as ham messages).\n",
    "\n",
    "The confusion matrices of the two classifiers are as follows \n",
    "\n",
    "> *Continue with parts **9.** and **10.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
